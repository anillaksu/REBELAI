# ==========================================
# ðŸ¤– REBEL AI Engine - YZ Entegrasyon KatmanÄ±
# ==========================================
# OpenAI, Ollama, Oobabooga ve Yerel Model DesteÄŸi

import os
import json
import yaml
import subprocess
import requests
import platform
from typing import Optional, Tuple, Dict, Any
from openai import OpenAI


class REBELAIEngine:
    def __init__(self, config_path: str = "rebel_config.yaml"):
        """REBEL AI Engine baÅŸlatÄ±cÄ±"""
        self.config = self._load_config(config_path)
        self.ai_config = self.config.get('ai_engine', {})
        self.platform_name = platform.system().lower()
        
        # AI istemcilerini baÅŸlat
        self.openai_client = self._init_openai()
        self.ollama_enabled = self.ai_config.get('ollama', {}).get('enabled', False)
        self.oobabooga_enabled = self.ai_config.get('oobabooga', {}).get('enabled', False)
        self.local_model_enabled = self.ai_config.get('local_model', {}).get('enabled', False)
        
        print(f"ðŸ¤– REBEL AI Engine initialized for {self.platform_name}")
    
    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """YAML yapÄ±landÄ±rma dosyasÄ±nÄ± yÃ¼kle"""
        try:
            with open(config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except Exception as e:
            print(f"âš ï¸ Config yÃ¼kleme hatasÄ±: {e}")
            return {}
    
    def _init_openai(self) -> Optional[OpenAI]:
        """OpenAI istemcisini baÅŸlat"""
        try:
            api_key = os.environ.get(
                self.ai_config.get('openai', {}).get('api_key_env', 'OPENAI_API_KEY')
            )
            if api_key:
                return OpenAI(api_key=api_key)
            else:
                print("âš ï¸ OpenAI API key bulunamadÄ±")
                return None
        except Exception as e:
            print(f"âš ï¸ OpenAI baÅŸlatma hatasÄ±: {e}")
            return None
    
    def interpret_command(self, user_input: str) -> Tuple[str, str, bool]:
        """
        KullanÄ±cÄ± girdisini yorumla ve platforma uygun komuta Ã§evir
        
        Returns:
            Tuple[interpreted_command, explanation, is_confident]
        """
        try:
            # Ã–nce OpenAI dene
            if self.openai_client:
                return self._interpret_with_openai(user_input)
            
            # Ollama dene
            elif self.ollama_enabled:
                return self._interpret_with_ollama(user_input)
            
            # Oobabooga dene
            elif self.oobabooga_enabled:
                return self._interpret_with_oobabooga(user_input)
            
            # Yerel model dene
            elif self.local_model_enabled:
                return self._interpret_with_local_model(user_input)
            
            # AI yok, temel Ã§eviri dene
            else:
                return self._interpret_basic(user_input)
                
        except Exception as e:
            print(f"âš ï¸ Komut yorumlama hatasÄ±: {e}")
            return user_input, f"âŒ Hata: {str(e)}", False
    
    def _interpret_with_openai(self, user_input: str) -> Tuple[str, str, bool]:
        """OpenAI ile komut yorumlama"""
        try:
            platform_shell = self._get_platform_shell()
            
            system_prompt = f"""Sen REBEL AI komut yorumlayÄ±cÄ±sÄ±sÄ±n. KullanÄ±cÄ±nÄ±n TÃ¼rkÃ§e/Ä°ngilizce komutunu {self.platform_name} ({platform_shell}) iÃ§in uygun shell komutuna Ã§evir.

KURALLAR:
1. Sadece gÃ¼venli, zararsÄ±z komutlar Ã¶ner
2. Emin deÄŸilsen "âš ï¸ Bu komutu doÄŸru anlamadÄ±m" diye baÅŸla
3. YanÄ±t formatÄ±: JSON {{"command": "shell_komutu", "explanation": "aÃ§Ä±klama", "confident": true/false}}
4. Tehlikeli komutlarÄ± (rm, sudo, etc.) asla Ã¶nerme
5. Platform: {self.platform_name}, Shell: {platform_shell}

Ã–RNEKLER:
- "dosyalarÄ± listele" â†’ "ls -la" (Linux/Mac) veya "dir" (Windows)
- "ben kimim" â†’ "whoami"
- "sistem bilgisi" â†’ "uname -a" (Linux/Mac) veya "systeminfo" (Windows)"""

            response = self.openai_client.chat.completions.create(
                model=self.ai_config.get('openai', {}).get('model', 'gpt-4o-mini'),
                messages=[
                    {"role": "system", "content": system_prompt},
                    {"role": "user", "content": user_input}
                ],
                max_tokens=self.ai_config.get('openai', {}).get('max_tokens', 1000),
                temperature=self.ai_config.get('openai', {}).get('temperature', 0.3),
                response_format={"type": "json_object"}
            )
            
            result = json.loads(response.choices[0].message.content)
            return (
                result.get('command', user_input),
                result.get('explanation', 'AI yorumlamasÄ±'),
                result.get('confident', False)
            )
            
        except Exception as e:
            print(f"âš ï¸ OpenAI API hatasÄ±: {e}")
            return user_input, f"âŒ OpenAI hatasÄ±: {str(e)}", False
    
    def _interpret_with_ollama(self, user_input: str) -> Tuple[str, str, bool]:
        """Ollama ile komut yorumlama"""
        try:
            endpoint = self.ai_config.get('ollama', {}).get('endpoint', 'http://localhost:11434')
            model = self.ai_config.get('ollama', {}).get('model', 'llama2')
            
            prompt = f"""KullanÄ±cÄ± komutu: "{user_input}"
Platform: {self.platform_name}
Bu komutu {self.platform_name} shell komutuna Ã§evir. Sadece gÃ¼venli komutlar Ã¶ner.
JSON formatÄ±nda yanÄ±t ver: {{"command": "shell_komutu", "explanation": "aÃ§Ä±klama", "confident": true/false}}"""

            response = requests.post(
                f"{endpoint}/api/generate",
                json={
                    "model": model,
                    "prompt": prompt,
                    "stream": False
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result_text = response.json().get('response', '{}')
                try:
                    result = json.loads(result_text)
                    return (
                        result.get('command', user_input),
                        result.get('explanation', 'Ollama yorumlamasÄ±'),
                        result.get('confident', False)
                    )
                except json.JSONDecodeError:
                    return user_input, "âŒ Ollama JSON parse hatasÄ±", False
            else:
                return user_input, f"âŒ Ollama baÄŸlantÄ± hatasÄ±: {response.status_code}", False
                
        except Exception as e:
            print(f"âš ï¸ Ollama hatasÄ±: {e}")
            return user_input, f"âŒ Ollama hatasÄ±: {str(e)}", False
    
    def _interpret_with_oobabooga(self, user_input: str) -> Tuple[str, str, bool]:
        """Oobabooga ile komut yorumlama"""
        try:
            endpoint = self.ai_config.get('oobabooga', {}).get('endpoint', 'http://localhost:5000')
            
            prompt = f"""KullanÄ±cÄ± komutu: "{user_input}"
Platform: {self.platform_name}
Bu komutu gÃ¼venli shell komutuna Ã§evir.
JSON: {{"command": "shell_komutu", "explanation": "aÃ§Ä±klama", "confident": true/false}}"""

            response = requests.post(
                f"{endpoint}/api/v1/generate",
                json={
                    "prompt": prompt,
                    "max_new_tokens": 200,
                    "temperature": 0.3,
                    "stop": ["\n\n"]
                },
                timeout=30
            )
            
            if response.status_code == 200:
                result_text = response.json().get('results', [{}])[0].get('text', '{}')
                try:
                    result = json.loads(result_text)
                    return (
                        result.get('command', user_input),
                        result.get('explanation', 'Oobabooga yorumlamasÄ±'),
                        result.get('confident', False)
                    )
                except json.JSONDecodeError:
                    return user_input, "âŒ Oobabooga JSON parse hatasÄ±", False
            else:
                return user_input, f"âŒ Oobabooga baÄŸlantÄ± hatasÄ±: {response.status_code}", False
                
        except Exception as e:
            print(f"âš ï¸ Oobabooga hatasÄ±: {e}")
            return user_input, f"âŒ Oobabooga hatasÄ±: {str(e)}", False
    
    def _interpret_with_local_model(self, user_input: str) -> Tuple[str, str, bool]:
        """Yerel model ile komut yorumlama"""
        try:
            model_path = self.ai_config.get('local_model', {}).get('model_path')
            executable_path = self.ai_config.get('local_model', {}).get('executable_path')
            context_size = self.ai_config.get('local_model', {}).get('context_size', 2048)
            
            if not os.path.exists(model_path) or not os.path.exists(executable_path):
                return user_input, "âŒ Yerel model dosyalarÄ± bulunamadÄ±", False
            
            prompt = f"""KullanÄ±cÄ± komutu: "{user_input}"
Platform: {self.platform_name}
GÃ¼venli shell komutuna Ã§evir.
JSON: {{"command": "shell_komutu", "explanation": "aÃ§Ä±klama", "confident": true/false}}"""

            # llama.cpp Ã§alÄ±ÅŸtÄ±r
            process = subprocess.run([
                executable_path,
                "-m", model_path,
                "-c", str(context_size),
                "-p", prompt,
                "--temp", "0.3",
                "-n", "200"
            ], capture_output=True, text=True, timeout=60)
            
            if process.returncode == 0:
                output = process.stdout.strip()
                try:
                    # JSON kÄ±smÄ±nÄ± Ã§Ä±kar
                    json_start = output.find('{')
                    json_end = output.rfind('}') + 1
                    if json_start >= 0 and json_end > json_start:
                        result = json.loads(output[json_start:json_end])
                        return (
                            result.get('command', user_input),
                            result.get('explanation', 'Yerel model yorumlamasÄ±'),
                            result.get('confident', False)
                        )
                except json.JSONDecodeError:
                    pass
                
                return user_input, "âŒ Yerel model JSON parse hatasÄ±", False
            else:
                return user_input, f"âŒ Yerel model hatasÄ±: {process.stderr}", False
                
        except Exception as e:
            print(f"âš ï¸ Yerel model hatasÄ±: {e}")
            return user_input, f"âŒ Yerel model hatasÄ±: {str(e)}", False
    
    def _interpret_basic(self, user_input: str) -> Tuple[str, str, bool]:
        """Temel yorumlama (AI olmadan)"""
        # Basit Ã§eviriler
        translations = {
            # TÃ¼rkÃ§e Ã§eviriler
            "dosyalarÄ± listele": "ls -la" if self.platform_name != "windows" else "dir",
            "dosya listesi": "ls -la" if self.platform_name != "windows" else "dir",
            "ben kimim": "whoami",
            "kim": "whoami",
            "nereyim": "pwd",
            "konum": "pwd",
            "tarih": "date",
            "saat": "date",
            "sistem bilgisi": "uname -a" if self.platform_name != "windows" else "systeminfo",
            "Ã§alÄ±ÅŸma sÃ¼resi": "uptime",
            "uptime": "uptime",
            "disk kullanÄ±mÄ±": "df -h" if self.platform_name != "windows" else "wmic logicaldisk get size,freespace,caption",
            "bellek kullanÄ±mÄ±": "free -h" if self.platform_name != "windows" else "wmic OS get TotalVisibleMemorySize,FreePhysicalMemory",
            "process listesi": "ps aux" if self.platform_name != "windows" else "tasklist",
            "sÃ¼reÃ§ listesi": "ps aux" if self.platform_name != "windows" else "tasklist",
            
            # Ä°ngilizce Ã§eviriler
            "list files": "ls -la" if self.platform_name != "windows" else "dir",
            "who am i": "whoami",
            "where am i": "pwd",
            "current directory": "pwd",
            "date and time": "date",
            "system info": "uname -a" if self.platform_name != "windows" else "systeminfo",
            "disk usage": "df -h" if self.platform_name != "windows" else "wmic logicaldisk get size,freespace,caption",
            "memory usage": "free -h" if self.platform_name != "windows" else "wmic OS get TotalVisibleMemorySize,FreePhysicalMemory",
            "process list": "ps aux" if self.platform_name != "windows" else "tasklist"
        }
        
        user_lower = user_input.lower().strip()
        
        for key, value in translations.items():
            if key in user_lower:
                return value, f"Temel Ã§eviri: '{key}' â†’ '{value}'", True
        
        # Bilinmeyen komut
        return user_input, "âš ï¸ Bu komutu doÄŸru anlamadÄ±m, ne yapmak istiyorsun?", False
    
    def _get_platform_shell(self) -> str:
        """Platform iÃ§in varsayÄ±lan shell'i al"""
        platform_config = self.config.get('platform', {})
        
        if self.platform_name == "windows":
            return platform_config.get('windows', {}).get('shell', 'powershell')
        elif self.platform_name == "linux":
            return platform_config.get('linux', {}).get('shell', 'bash')
        elif self.platform_name == "darwin":  # macOS
            return platform_config.get('macos', {}).get('shell', 'zsh')
        else:
            return "bash"
    
    def analyze_error(self, command: str, error_output: str) -> str:
        """Hata Ã§Ä±ktÄ±sÄ±nÄ± analiz et ve Ã§Ã¶zÃ¼m Ã¶nerisi sun"""
        try:
            if self.openai_client:
                system_prompt = f"""Sen REBEL AI hata analizcisisÄ±n. {self.platform_name} sisteminde Ã§alÄ±ÅŸan komutlarÄ±n hatalarÄ±nÄ± analiz et ve Ã§Ã¶zÃ¼m Ã¶ner.

Komut: {command}
Hata: {error_output}

TÃ¼rkÃ§e olarak:
1. HatanÄ±n sebebini aÃ§Ä±kla
2. Ã‡Ã¶zÃ¼m Ã¶nerileri sun
3. Alternatif komutlar Ã¶ner"""

                response = self.openai_client.chat.completions.create(
                    model=self.ai_config.get('openai', {}).get('model', 'gpt-4o-mini'),
                    messages=[
                        {"role": "system", "content": system_prompt}
                    ],
                    max_tokens=500,
                    temperature=0.3
                )
                
                return response.choices[0].message.content
            else:
                return f"âŒ Hata bulundu: {error_output}\nðŸ’¡ Ä°pucu: Komut sÃ¶zdizimini kontrol edin veya yetki gerekebilir."
                
        except Exception as e:
            return f"âŒ Hata analizi yapÄ±lamadÄ±: {str(e)}"
    
    def get_ai_status(self) -> Dict[str, Any]:
        """AI motorlarÄ±nÄ±n durumunu dÃ¶ndÃ¼r"""
        return {
            "openai_available": self.openai_client is not None,
            "ollama_enabled": self.ollama_enabled,
            "oobabooga_enabled": self.oobabooga_enabled,
            "local_model_enabled": self.local_model_enabled,
            "platform": self.platform_name,
            "shell": self._get_platform_shell()
        }


# Test fonksiyonu
if __name__ == "__main__":
    engine = REBELAIEngine()
    
    print("ðŸ¤– REBEL AI Engine Test")
    print("=" * 40)
    
    test_commands = [
        "dosyalarÄ± listele",
        "ben kimim",
        "sistem bilgisi",
        "list files",
        "bilinmeyen komut test"
    ]
    
    for cmd in test_commands:
        result, explanation, confident = engine.interpret_command(cmd)
        print(f"Girdi: {cmd}")
        print(f"Ã‡Ä±ktÄ±: {result}")
        print(f"AÃ§Ä±klama: {explanation}")
        print(f"GÃ¼venilir: {confident}")
        print("-" * 30)
    
    print("\nðŸ“Š AI Durum:")
    status = engine.get_ai_status()
    for key, value in status.items():
        print(f"{key}: {value}")